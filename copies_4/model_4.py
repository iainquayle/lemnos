import torch
class M4(torch.nn.Module):
	def __init__(self):
		super().__init__()
		import torch
		self.c0000_1 = torch.nn.Conv1d(56, 15, (1,), (1,), (0,), (1,), 1, bias=True, padding_mode='zeros')
		self.c0000_2 = torch.nn.ReLU6()
		self.c0000_3 = torch.nn.BatchNorm1d(15)
		self.c0001_1 = torch.nn.BatchNorm1d(15)
		self.c0002_1 = torch.nn.Conv1d(15, 16, (2,), (2,), (0,), (1,), 1, bias=True, padding_mode='zeros')
		self.c0002_2 = torch.nn.SiLU()
		self.c0002_3 = torch.nn.BatchNorm1d(16)
		self.c0003_1 = torch.nn.BatchNorm1d(16)
		self.c0004_1 = torch.nn.Conv1d(16, 28, (2,), (2,), (0,), (1,), 1, bias=True, padding_mode='zeros')
		self.c0004_2 = torch.nn.SiLU()
		self.c0004_3 = torch.nn.BatchNorm1d(28)
		self.c0005_1 = torch.nn.BatchNorm1d(28)
		self.c0006_1 = torch.nn.Conv1d(28, 27, (2,), (2,), (0,), (1,), 1, bias=True, padding_mode='zeros')
		self.c0006_2 = torch.nn.SiLU()
		self.c0006_3 = torch.nn.BatchNorm1d(27)
		self.c0007_1 = torch.nn.BatchNorm1d(27)
		self.c0008_1 = torch.nn.Conv1d(27, 29, (2,), (2,), (0,), (1,), 1, bias=True, padding_mode='zeros')
		self.c0008_2 = torch.nn.SiLU()
		self.c0008_3 = torch.nn.BatchNorm1d(29)
		self.c0009_1 = torch.nn.BatchNorm1d(29)
		self.c000a_1 = torch.nn.Conv1d(29, 16, (2,), (2,), (0,), (1,), 1, bias=True, padding_mode='zeros')
		self.c000a_2 = torch.nn.SiLU()
		self.c000a_3 = torch.nn.BatchNorm1d(16)
		self.c000b_1 = torch.nn.BatchNorm1d(16)
		self.c000c_1 = torch.nn.Conv1d(16, 31, (2,), (2,), (0,), (1,), 1, bias=True, padding_mode='zeros')
		self.c000c_2 = torch.nn.SiLU()
		self.c000c_3 = torch.nn.BatchNorm1d(31)
		self.c000d_1 = torch.nn.BatchNorm1d(31)
		self.c000e_1 = torch.nn.Conv1d(31, 16, (2,), (2,), (0,), (1,), 1, bias=True, padding_mode='zeros')
		self.c000e_2 = torch.nn.SiLU()
		self.c000e_3 = torch.nn.BatchNorm1d(16)
		self.c000f_1 = torch.nn.BatchNorm1d(16)
		self.c0010_1 = torch.nn.Conv1d(16, 16, (2,), (2,), (0,), (1,), 1, bias=True, padding_mode='zeros')
		self.c0010_2 = torch.nn.SiLU()
		self.c0010_3 = torch.nn.BatchNorm1d(16)
		self.c0011_1 = torch.nn.BatchNorm1d(16)
		self.c0012_1 = torch.nn.Linear(16, 1, bias=True)
	def forward(self, r0001):
		import torch
		r0001 = r0001.view(-1, 14336)
		r0001 = self.c0000_3(self.c0000_2(self.c0000_1(r0001.view(-1, 56, 256)))).view(-1, 3840)
		r0001 = self.c0001_1(r0001.view(-1, 15, 256)).view(-1, 3840)
		r0001 = self.c0002_3(self.c0002_2(self.c0002_1(r0001.view(-1, 15, 256)))).view(-1, 2048)
		r0001 = self.c0003_1(r0001.view(-1, 16, 128)).view(-1, 2048)
		r0001 = self.c0004_3(self.c0004_2(self.c0004_1(r0001.view(-1, 16, 128)))).view(-1, 1792)
		r0001 = self.c0005_1(r0001.view(-1, 28, 64)).view(-1, 1792)
		r0001 = self.c0006_3(self.c0006_2(self.c0006_1(r0001.view(-1, 28, 64)))).view(-1, 864)
		r0001 = self.c0007_1(r0001.view(-1, 27, 32)).view(-1, 864)
		r0001 = self.c0008_3(self.c0008_2(self.c0008_1(r0001.view(-1, 27, 32)))).view(-1, 464)
		r0001 = self.c0009_1(r0001.view(-1, 29, 16)).view(-1, 464)
		r0001 = self.c000a_3(self.c000a_2(self.c000a_1(r0001.view(-1, 29, 16)))).view(-1, 128)
		r0001 = self.c000b_1(r0001.view(-1, 16, 8)).view(-1, 128)
		r0001 = self.c000c_3(self.c000c_2(self.c000c_1(r0001.view(-1, 16, 8)))).view(-1, 124)
		r0001 = self.c000d_1(r0001.view(-1, 31, 4)).view(-1, 124)
		r0001 = self.c000e_3(self.c000e_2(self.c000e_1(r0001.view(-1, 31, 4)))).view(-1, 32)
		r0001 = self.c000f_1(r0001.view(-1, 16, 2)).view(-1, 32)
		r0001 = self.c0010_3(self.c0010_2(self.c0010_1(r0001.view(-1, 16, 2)))).view(-1, 16)
		r0001 = self.c0011_1(r0001.view(-1, 16, 1)).view(-1, 16)
		r0001 = self.c0012_1(r0001)
		return r0001